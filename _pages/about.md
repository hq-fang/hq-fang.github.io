---
permalink: /
title: "About"
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

<span id="handwave">üëãüèª</span> Hi! I'm an undergraduate student and researcher at the [University of Washington (UW)](https://washington.edu/). I double major in computer science (with honors) and statistics, with a minor in mathematics. I am also a student researcher working with the [Perceptual Reasoning and Interaction Research (PRIOR)](https://prior.allenai.org/) team at the [Allen Institute for AI (Ai2)](https://allenai.org/).

At UW, I am fortunate to be co-advised by Prof [Ranjay Krishna](https://www.ranjaykrishna.com/) at the [Reasoning, AI, and VisioN (RAIVN) Lab](https://raivn.cs.washington.edu/) and Prof [Dieter Fox](https://homes.cs.washington.edu/~fox/) at the [Robotics and State Estimation (RSE) Lab](https://rse-lab.cs.washington.edu/) in UW [CSE](https://cs.washington.edu/). Before that, I was advised by Prof [Jenq-Neng Hwang](https://people.ece.uw.edu/hwang/) at the [Information Processing Lab (IPL)](https://ipl-uw.github.io/) in UW [ECE](https://www.ece.uw.edu/). I am also pleased to be mentored by Dr [Jiafei Duan](https://duanjiafei.com/) and Dr [Ying Jin](https://scholar.google.com/citations?user=MNrLDhwAAAAJ&hl=en/). At Ai2 PRIOR, I am fortunate to be mentored by Prof [Ranjay Krishna](https://www.ranjaykrishna.com/), Prof [Ali Farhadi](https://homes.cs.washington.edu/~ali/), and Dr [Jiafei Duan](https://duanjiafei.com/), and sponsored by Dr [Rose Hendrix](https://rosehendrix.com/).

My research interests lie in robot learning and foundation models for robotics. In particular, I develop generalist robotic manipulation policies with spatial intelligence, enabling them to perceive, remember, and reason in space.

**I am now actively seeking PhD positions for fall 2026 and any other opportunities. Feel free to reach out!**

## üî• News {#news}
- **[Oct 2025]** Our paper *MolmoAct: Action Reasoning Models that can Reason in Space* has been featured by [State of AI Report 2025](https://docs.google.com/presentation/d/1xiLl0VdrlNMAei8pmaX4ojIOfej6lhvZbOIK7Z6C-Go/edit?slide=id.g379a362bd85_0_90#slide=id.g379a362bd85_0_90/).
- **[Oct 2025]** Our paper *FailSafe: Reasoning and Recovery from Failures in Vision-Language-Action Models* has been preprinted to arXiv.
- **[Sep 2025]** Our paper *SAM2Act: Integrating Visual Foundation Model with A Memory Architecture for Robotic Manipulation* has been accepted to [RemembeRL @ CoRL 2025](https://rememberl-corl25.github.io/) and selected as <span style="color:red;">Best Paper Award</span>.
- **[Sep 2025]** Our paper *MolmoAct: Action Reasoning Models that can Reason in Space* has been accepted to [Rational Robots @ CoRL 2025](https://rational-robots.github.io/) and selected as <span style="color:red;">Best Paper Award Runner-up</span>.
- **[Sep 2025]** Our paper *MolmoAct: Action Reasoning Models that can Reason in Space* has been accepted to [Data @ CoRL 2025](https://sites.google.com/stanford.edu/corldata25/) and selected as <span style="color:red;">Best Paper Award Nominee</span>.
- **[Aug 2025]** We are launching *[MolmoAct](https://allenai.org/blog/molmoact): Action Reasoning Models that can Reason in Space*.
- **[Jul 2025]** Received the UW [CSE Award for Excellence Scholarship](https://www.cs.washington.edu/academics/undergraduate/student-resources/scholarships-awards/#allen) 2025.
- **[Jun 2025]** Received the UW CSE [Best Senior Thesis Award](https://www.cs.washington.edu/academics/undergraduate/student-resources/scholarships-awards/senior-thesis-award/) Honorable Mention 2025 (2nd Place).
- **[May 2025]** Our paper *PointArena: Probing Multimodal Grounding Through Language-Guided Pointing* has been preprinted to arXiv.
- **[May 2025]** Our paper *SAM2Act: Integrating Visual Foundation Model with A Memory Architecture for Robotic Manipulation* has been accepted to [ICML 2025](https://icml.cc/).
- **[Apr 2025]** Our paper *SAM2Act: Integrating Visual Foundation Model with A Memory Architecture for Robotic Manipulation* has been featured by [AI Index Report 2025](https://hai.stanford.edu/ai-index/2025-ai-index-report/).

## üìë Selected Publications {#research}
<div class="paper-legend">
  <span><code>*</code> Equal Contribution as First Authors</span>
  <span><code>‚Ä†</code> Equal Contribution as Second Authors</span>
  <span><code>‚Ä°</code> Equal Contribution as Advisors</span>
</div>
<div class='paper-box'><div class='paper-box-image'><div><video id="2025molmoact" src='images/2025molmoact.mp4' alt="sym" width="100%" autoplay muted loop playsinline preload="auto" onloadedmetadata="this.defaultPlaybackRate=3;this.playbackRate=3" onplay="this.playbackRate=3" onratechange="if(this.playbackRate!==3)this.playbackRate=3"></video></div></div>
<div class='paper-box-text' markdown="1">
**MolmoAct: Action Reasoning Models that can Reason in Space**
<br>Jason Lee\*, Jiafei Duan\*, <ins>Haoquan Fang\*</ins>, Yuquan Deng, Shuo Liu, Boyang Li, Bohan Fang, Jieyu Zhang, Yi Ru Wang, Sangho Lee, Winson Han, Wilbert Pumacay, Angelica Wu, Rose Hendrix, Karen Farley, Eli VanderBilt, Ali Farhadi, Dieter Fox, Ranjay Krishna
<br>*Technical Report 2025*
<br>*Data @ CoRL 2025 <span style="color:red;">Best Paper Award Nominee</span>*
<br>*Rational Robots @ CoRL 2025 <span style="color:red;">Best Paper Award Runner-up</span>*
<br>
<a href="https://allenai.org/blog/molmoact" style="text-decoration:none !important; border-bottom:0 !important;">
  <img alt="Blog" src="https://img.shields.io/badge/Blog--white?style=social&logo=googlechrome">
</a>
<a href="https://api.semanticscholar.org/arXiv:2508.07917" style="text-decoration:none !important; border-bottom:0 !important;">
  <img alt="Paper" src="https://img.shields.io/badge/dynamic/json?style=social&logo=semanticscholar&label=Paper&url=https%3A%2F%2Fhq-fang.github.io%2Fcitations%2Fcites_2508.07917.json&query=%24&suffix=%20%E2%80%9D&cacheSeconds=86400&v=2">
</a>
<a href="https://github.com/allenai/molmoact" style="text-decoration:none !important; border-bottom:0 !important;">
  <img alt="Code"
       src="https://img.shields.io/badge/dynamic/json?style=social&label=Code&logo=github&query=%24.stargazers_count&url=https%3A%2F%2Fapi.github.com%2Frepos%2Fallenai%2Fmolmoact&suffix=%20‚≠ê&cacheSeconds=3600">
</a>
<a href="https://huggingface.co/collections/allenai/molmoact-689697591a3936fba38174d7" style="text-decoration:none !important; border-bottom:0 !important;">
  <img alt="Models" src="https://img.shields.io/badge/Models--white?style=social&logo=huggingface&logoColor=yellow">
</a>
<a href="https://huggingface.co/collections/allenai/molmoact-data-mixture-6897e583e13b6c2cf3ea2b80" style="text-decoration:none !important; border-bottom:0 !important;">
  <img alt="Datasets" src="https://img.shields.io/badge/Datasets--white?style=social&logo=huggingface&logoColor=yellow">
</a>
<a href="https://x.com/hq_fang/status/1955296974128718176" style="text-decoration:none !important; border-bottom:0 !important;">
  <img alt="Post" src="https://img.shields.io/badge/Post--white?style=social&logo=x">
</a>
<a href="https://docs.google.com/presentation/d/1xiLl0VdrlNMAei8pmaX4ojIOfej6lhvZbOIK7Z6C-Go/edit?slide=id.g379a362bd85_0_90#slide=id.g379a362bd85_0_90/" style="text-decoration:none !important; border-bottom:0 !important;">
  <img alt="State of AI Report" src="https://img.shields.io/badge/State of AI Report--white?style=social&logo=googledocs&&logoColor=313554">
</a>
</div>
</div>
<div class='paper-box'><div class='paper-box-image'><div><video id="2025sam2act" src='images/2025sam2act.mp4' alt="sym" width="100%" autoplay muted loop playsinline preload="auto" onloadedmetadata="this.defaultPlaybackRate=3;this.playbackRate=3" onplay="this.playbackRate=3" onratechange="if(this.playbackRate!==3)this.playbackRate=3"></video></div></div>
<div class='paper-box-text' markdown="1">
**SAM2Act: Integrating Visual Foundation Model with A Memory Architecture for Robotic Manipulation**
<br><ins>Haoquan Fang</ins>, Markus Grotz, Wilbert Pumacay, Yi Ru Wang, Dieter Fox<sup>‚Ä°</sup>, Ranjay Krishna<sup>‚Ä°</sup>, Jiafei Duan<sup>‚Ä°</sup>
<br>*International Conference on Machine Learning (ICML) 2025*
<br>*WRL @ ICLR 2025 <span style="color:red;">Oral Presentation</span>*
<br>*FMEA @ CVPR 2025 <span style="color:red;">Oral Presentation</span>*
<br>*RemembeRL @ CoRL 2025 <span style="color:red;">Best Paper Award</span>*
<br>
<a href="https://sam2act.github.io" style="text-decoration:none !important; border-bottom:0 !important;">
  <img alt="Website" src="https://img.shields.io/badge/Website--white?style=social&logo=googlechrome">
</a>
<a href="https://api.semanticscholar.org/arXiv:2501.18564" style="text-decoration:none !important; border-bottom:0 !important;">
  <img alt="Paper" src="https://img.shields.io/badge/dynamic/json?style=social&logo=semanticscholar&label=Paper&url=https%3A%2F%2Fhq-fang.github.io%2Fcitations%2Fcites_2501.18564.json&query=%24&suffix=%20%E2%80%9D&cacheSeconds=86400&v=2">
</a>
<a href="https://github.com/sam2act/sam2act" style="text-decoration:none !important; border-bottom:0 !important;">
  <img alt="Code"
       src="https://img.shields.io/badge/dynamic/json?style=social&label=Code&logo=github&query=%24.stargazers_count&url=https%3A%2F%2Fapi.github.com%2Frepos%2Fsam2act%2Fsam2act&suffix=%20‚≠ê&cacheSeconds=3600">
</a>
<a href="https://huggingface.co/datasets/hqfang/memorymench" style="text-decoration:none !important; border-bottom:0 !important;">
  <img alt="MemoryBench" src="https://img.shields.io/badge/MemoryBench--white?style=social&logo=huggingface&logoColor=yellow">
</a>
<a href="https://x.com/hq_fang/status/1885105252971798682" style="text-decoration:none !important; border-bottom:0 !important;">
  <img alt="Post" src="https://img.shields.io/badge/Post--white?style=social&logo=x">
</a>
<a href="https://hai-production.s3.amazonaws.com/files/hai_ai_index_report_2025.pdf#page=151" style="text-decoration:none !important; border-bottom:0 !important;">
  <img alt="AI Index Report" src="https://img.shields.io/badge/AI Index Report--white?style=social&logo=googledocs&&logoColor=313554">
</a>
</div>
</div>
<div class='paper-box'><div class='paper-box-image'><div><video id="2025pointarena" src='images/2025pointarena.mp4' alt="sym" width="100%" autoplay muted loop playsinline preload="auto" onloadedmetadata="this.defaultPlaybackRate=3;this.playbackRate=3" onplay="this.playbackRate=3" onratechange="if(this.playbackRate!==3)this.playbackRate=3"></video></div></div>
<div class='paper-box-text' markdown="1">
**PointArena: Probing Multimodal Grounding Through Language-Guided Pointing**
<br>Long Cheng\*, Jiafei Duan\*, Yi Ru Wang<sup>‚Ä†</sup>, <ins>Haoquan Fang<sup>‚Ä†</sup></ins>, Boyang Li<sup>‚Ä†</sup>, Yushan Huang, Elvis Wang, Ainaz Eftekhar, Jason Lee, Wentao Yuan, Rose Hendrix, Noah A. Smith, Fei Xia, Dieter Fox, Ranjay Krishna
<br>*arXiv 2025*
<br>
<a href="https://pointarena.github.io" style="text-decoration:none !important; border-bottom:0 !important;">
  <img alt="Website" src="https://img.shields.io/badge/Website--white?style=social&logo=googlechrome">
</a>
<a href="https://api.semanticscholar.org/arXiv:2505.09990" style="text-decoration:none !important; border-bottom:0 !important;">
  <img alt="Paper" src="https://img.shields.io/badge/dynamic/json?style=social&logo=semanticscholar&label=Paper&url=https%3A%2F%2Fhq-fang.github.io%2Fcitations%2Fcites_2505.09990.json&query=%24&suffix=%20%E2%80%9D&cacheSeconds=86400&v=2">
</a>
<a href="https://github.com/pointarena/pointarena" style="text-decoration:none !important; border-bottom:0 !important;">
  <img alt="Code"
       src="https://img.shields.io/badge/dynamic/json?style=social&label=Code&logo=github&query=%24.stargazers_count&url=https%3A%2F%2Fapi.github.com%2Frepos%2Fpointarena%2Fpointarena&suffix=%20‚≠ê&cacheSeconds=3600">
</a>
<a href="https://huggingface.co/datasets/PointArena/pointarena-data" style="text-decoration:none !important; border-bottom:0 !important;">
  <img alt="Dataset" src="https://img.shields.io/badge/Dataset--white?style=social&logo=huggingface&logoColor=yellow">
</a>
<a href="https://x.com/hq_fang/status/1924516643369107958" style="text-decoration:none !important; border-bottom:0 !important;">
  <img alt="Post" src="https://img.shields.io/badge/Post--white?style=social&logo=x">
</a>
</div>
</div>
<div class='paper-box'><div class='paper-box-image'><div><video id="2025failsafe" src='images/2025failsafe.mp4' alt="sym" width="100%" autoplay muted loop playsinline preload="auto" onloadedmetadata="this.defaultPlaybackRate=3;this.playbackRate=3" onplay="this.playbackRate=3" onratechange="if(this.playbackRate!==3)this.playbackRate=3"></video></div></div>
<div class='paper-box-text' markdown="1">
**FailSafe: Reasoning and Recovery from Failures in Vision-Language-Action Models**
<br>Zijun Lin, Jiafei Duan, <ins>Haoquan Fang</ins>, Dieter Fox, Ranjay Krishna, Cheston Tan, Bihan Wen
<br>*arXiv 2025*
<br>
<a href="https://jimntu.github.io/FailSafe/" style="text-decoration:none !important; border-bottom:0 !important;">
  <img alt="Website" src="https://img.shields.io/badge/Website--white?style=social&logo=googlechrome">
</a>
<!-- <a href="https://api.semanticscholar.org/arXiv:2505.09990" style="text-decoration:none !important; border-bottom:0 !important;">
  <img alt="Paper" src="https://img.shields.io/badge/dynamic/json?style=social&logo=semanticscholar&label=Paper&url=https%3A%2F%2Fhq-fang.github.io%2Fcitations%2Fcites_2505.09990.json&query=%24&suffix=%20%E2%80%9D&cacheSeconds=86400&v=2">
</a> -->
<a href="https://api.semanticscholar.org/arXiv:2510.01642" style="text-decoration:none !important; border-bottom:0 !important;">
  <img alt="Paper" src="https://img.shields.io/badge/Paper--white?style=social&logo=semanticscholar">
</a>
</div>
</div>

## üìñ Academic Services {#services}
- **Conference Reviewer**
  - International Conference on Learning Representations (ICLR) 2025 Workshops
  - Conference on Computer Vision and Pattern Recognition (CVPR) 2025 Workshops
  - International Conference on Robotics and Automation (ICRA) 2026
- **Workshop Organizer**
  - Workshop on 3D Vision Language Models for Robotic Manipulation ([Robo-3DVLM](https://robo-3dvlms.github.io/)) at CVPR 2025
- **Teaching Assistant**
  - CSE 493G1/599G1: Deep Learning ([Spring 2025](https://courses.cs.washington.edu/courses/cse493g1/25sp/), Undergrad/Grad)
- **Grader**
  - STAT 311: Elements of Statistical Methods (Autumn 2025)

## üí¨ Presentations {#talks}
All presentations below were delivered in person by me, either solo or with coauthors.
- **Genesis AI**, *Invited Talk on MolmoAct*
  - Hosted by [Johnson Wang](https://zswang666.github.io/), Aug 2025, San Carlos, CA
- **Google Deepmind**, *Invited Talk on MolmoAct*
  - Hosted by [Ted Xiao](https://tedxiao.me/), Aug 2025, Mountain View, CA
- **Allen Institute for AI**, *Company Demo on MolmoAct*
  - Hosted by [Ali Farhadi](https://www.cs.washington.edu/people/faculty/ali-farhadi/), Jun 2025, Seattle, WA
- **FMEA @ CVPR 2025**, *Oral Presentation on SAM2Act*
  - Hosted by [Manling Li](https://limanling.github.io/), Jun 2025, Nashville, TN
- **UW RAIVN Lab**, *Group Talk on SAM2Act*
  - Hosted by [Ranjay Krishna](https://ranjaykrishna.com/), Feb 2025, Seattle, WA
- **UW RSE Lab**, *Group Talk on SAM2Act*
  - Hosted by [Dieter Fox](https://homes.cs.washington.edu/~fox/), Feb 2025, Seattle, WA

## üéì Education {#education}
- **University of Washington**, *2022 - 2026 (Expected)*
<br>*Bachelor of Science in Computer Science with Honors*
<br>*Bachelor of Science in Statistics*
<br>*Minor in Mathematics*
  - GPA: 3.98/4.00, Dean‚Äôs Lists, Best Senior Thesis Award Honorable Mention, CSE Award for Excellence Scholarship

## üíº Industrial Experiences {#work}
- **Allen Institute for AI**
<br>*Student Researcher, Nov 2025 - Present*
<br>*Student Collaborator, Apr 2025 - Oct 2025*
  - Worked with the [Perceptual Reasoning and Interaction Research (PRIOR)](https://prior.allenai.org/) Team
  - Mentored by Prof Ranjay Krishna, Prof Ali Farhadi, and Dr Jiafei Duan, Sponsored by Dr Rose Hendrix
- **Nokia Shanghai Bell**
<br>*Research Engineer Intern, Jul 2023 - Aug 2023*
  - Mentored by Dr Quan Wang, Explored on [COSMIC](https://cosmic-sizing.org/) Report Generation

## üèÖ Awards {#awards}
- Best Paper Award at [RemembeRL @ CoRL 2025](https://rememberl-corl25.github.io/)
- Best Paper Award Runner-up at [Rational Robots @ CoRL 2025](https://rational-robots.github.io/)
- Best Paper Award Nominee at [Data @ CoRL 2025](https://sites.google.com/stanford.edu/corldata25/)
- UW [CSE Award for Excellence Scholarship](https://www.cs.washington.edu/academics/undergraduate/student-resources/scholarships-awards/#allen) 2025
- UW CSE [Best Senior Thesis Award](https://www.cs.washington.edu/academics/undergraduate/student-resources/scholarships-awards/senior-thesis-award/) Honorable Mention 2025 (2nd Place)
- Kaggle Competitions Expert (Top 0.85% Highest)
- Kaggle - LLM Science Exam, Silver Medal (Top 4%) [[Website]](https://www.kaggle.com/competitions/kaggle-llm-science-exam/discussion/446303/)
- RSNA Screening Mammography Breast Cancer Detection, Solo Silver Medal (Top 5%)
