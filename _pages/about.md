---
permalink: /
title: "About"
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

<span id="handwave">üëãüèª</span> Hi! I'm an undergraduate student and researcher at the [University of Washington (UW)](https://washington.edu/). I double major in computer science (with honors) and statistics, with a minor in mathematics. I am also a student researcher working with the [Perceptual Reasoning and Interaction Research (PRIOR)](https://prior.allenai.org/) team at the [Allen Institute for AI (Ai2)](https://allenai.org/).

At UW, I am fortunate to be advised by Prof [Ranjay Krishna](https://www.ranjaykrishna.com/) at the [Reasoning, AI, and VisioN (RAIVN) Lab](https://raivn.cs.washington.edu/) in UW [CSE](https://cs.washington.edu/). Before that, I was privileged to be advised by Prof [Jenq-Neng Hwang](https://people.ece.uw.edu/hwang/) at the [Information Processing Lab (IPL)](https://ipl-uw.github.io/) in UW [ECE](https://www.ece.uw.edu/). At Ai2 PRIOR, I am fortunate to be co-advised by Prof [Ranjay Krishna](https://www.ranjaykrishna.com/) and Prof [Ali Farhadi](https://homes.cs.washington.edu/~ali/). I am also pleased to be mentored by and collaborate with Prof [Dieter Fox](https://homes.cs.washington.edu/~fox/), Dr [Jiafei Duan](https://duanjiafei.com/), and Dr [Ying Jin](https://scholar.google.com/citations?user=MNrLDhwAAAAJ&hl=en/).

My research interests lie broadly in robot learning. In particular, I focus on developing generalist robotic manipulation policies that leverage strong priors, by optimizing both the data curation and model architectures.

**I am now actively seeking PhD positions for fall 2026 and any other opportunities. Feel free to reach out!**

## üî• News {#news}
- **[Feb 2026]** Our paper *Recurrent-Depth VLA: Implicit Test-Time Compute Scaling of Vision-Language-Action Models via Latent Iterative Reasoning* has been preprinted to arXiv.
- **[Jan 2026]** Our paper *MolmoAct: Action Reasoning Models that can Reason in Space* has been accepted to [ICRA 2026](https://2026.ieee-icra.org/).
- **[Dec 2025]** Honored to be selected as the [CRA Outstanding Undergraduate Researcher Award](https://cra.org/crn/2026/01/2025-26-cra-outstanding-undergraduate-researcher-award-recipients-announced/) Awardee 2026 (8 Awardees in North America)!
- **[Oct 2025]** Our paper *MolmoAct: Action Reasoning Models that can Reason in Space* has been featured by [State of AI Report 2025](https://docs.google.com/presentation/d/1xiLl0VdrlNMAei8pmaX4ojIOfej6lhvZbOIK7Z6C-Go/edit?slide=id.g379a362bd85_0_90#slide=id.g379a362bd85_0_90/).
- **[Oct 2025]** Our paper *FailSafe: Reasoning and Recovery from Failures in Vision-Language-Action Models* has been preprinted to arXiv.
- **[Sep 2025]** Our paper *SAM2Act: Integrating Visual Foundation Model with A Memory Architecture for Robotic Manipulation* has been accepted to [RemembeRL @ CoRL 2025](https://rememberl-corl25.github.io/) and selected as <span style="color:red;">Best Paper Award</span>.
- **[Sep 2025]** Our paper *MolmoAct: Action Reasoning Models that can Reason in Space* has been accepted to [Rational Robots @ CoRL 2025](https://rational-robots.github.io/) and selected as <span style="color:red;">Best Paper Award Runner-up</span>.
- **[Aug 2025]** We are launching *[MolmoAct](https://allenai.org/blog/molmoact): Action Reasoning Models that can Reason in Space*.
- **[Jul 2025]** Received the UW [CSE Award for Excellence Scholarship](https://www.cs.washington.edu/academics/undergraduate/student-resources/scholarships-awards/#allen) 2025.
- **[Jun 2025]** Received the UW CSE [Best Senior Thesis Award](https://www.cs.washington.edu/academics/undergraduate/student-resources/scholarships-awards/senior-thesis-award/) Honorable Mention 2025 (2nd Place).
- **[May 2025]** Our paper *PointArena: Probing Multimodal Grounding Through Language-Guided Pointing* has been preprinted to arXiv.
- **[May 2025]** Our paper *SAM2Act: Integrating Visual Foundation Model with A Memory Architecture for Robotic Manipulation* has been accepted to [ICML 2025](https://icml.cc/).
- **[Apr 2025]** Our paper *SAM2Act: Integrating Visual Foundation Model with A Memory Architecture for Robotic Manipulation* has been featured by [AI Index Report 2025](https://hai.stanford.edu/ai-index/2025-ai-index-report/).

## üìë Selected Publications {#research}
<div class="paper-legend">
  Equal Contribution as<span><code>*</code> First Authors</span>
  <span><code>‚Ä†</code> Second Authors</span>
  <span><code>‚Ä°</code> Advisors</span>
</div>
<div class='paper-box'><div class='paper-box-image'><div><video id="2025molmoact" src='images/2025molmoact.mp4' alt="sym" width="100%" autoplay muted loop playsinline preload="auto" onloadedmetadata="this.defaultPlaybackRate=3;this.playbackRate=3" onplay="this.playbackRate=3" onratechange="if(this.playbackRate!==3)this.playbackRate=3"></video></div></div>
<div class='paper-box-text' markdown="1">
**MolmoAct: Action Reasoning Models that can Reason in Space**
<br>Jason Lee\*, Jiafei Duan\*, <ins>Haoquan Fang\*</ins>, Yuquan Deng, Shuo Liu, Boyang Li, Bohan Fang, Jieyu Zhang, Yi Ru Wang, Sangho Lee, Winson Han, Wilbert Pumacay, Angelica Wu, Rose Hendrix, Karen Farley, Eli VanderBilt, Ali Farhadi, Dieter Fox, Ranjay Krishna
<br>*International Conference on Robotics and Automation (ICRA) 2026*
<br>*Data @ CoRL 2025 <span style="color:red;">Best Paper Award Nominee</span>*
<br>*Rational Robots @ CoRL 2025 <span style="color:red;">Best Paper Award Runner-up</span>*
<br>
<a href="https://allenai.org/blog/molmoact" style="text-decoration:none !important; border-bottom:0 !important;">
  <img alt="Blog" src="https://img.shields.io/badge/Blog--white?style=social&logo=googlechrome">
</a>
<a href="https://api.semanticscholar.org/arXiv:2508.07917" style="text-decoration:none !important; border-bottom:0 !important;">
  <img alt="Paper" src="https://img.shields.io/badge/dynamic/json?style=social&logo=semanticscholar&label=Paper&url=https%3A%2F%2Fhq-fang.github.io%2Fcitations%2Fcites_2508.07917.json&query=%24&suffix=%20%E2%80%9D&cacheSeconds=86400&v=2">
</a>
<a href="https://github.com/allenai/molmoact" style="text-decoration:none !important; border-bottom:0 !important;">
  <img alt="Code"
       src="https://img.shields.io/badge/dynamic/json?style=social&label=Code&logo=github&query=%24.stargazers_count&url=https%3A%2F%2Fapi.github.com%2Frepos%2Fallenai%2Fmolmoact&suffix=%20‚≠ê&cacheSeconds=3600">
</a>
<a href="https://huggingface.co/collections/allenai/molmoact-689697591a3936fba38174d7" style="text-decoration:none !important; border-bottom:0 !important;">
  <img alt="Models" src="https://img.shields.io/badge/Models--white?style=social&logo=huggingface&logoColor=yellow">
</a>
<a href="https://huggingface.co/collections/allenai/molmoact-data-mixture-6897e583e13b6c2cf3ea2b80" style="text-decoration:none !important; border-bottom:0 !important;">
  <img alt="Datasets" src="https://img.shields.io/badge/Datasets--white?style=social&logo=huggingface&logoColor=yellow">
</a>
<a href="https://x.com/hq_fang/status/1955296974128718176" style="text-decoration:none !important; border-bottom:0 !important;">
  <img alt="Post" src="https://img.shields.io/badge/Post--white?style=social&logo=x">
</a>
<a href="https://docs.google.com/presentation/d/1xiLl0VdrlNMAei8pmaX4ojIOfej6lhvZbOIK7Z6C-Go/edit?slide=id.g379a362bd85_0_90#slide=id.g379a362bd85_0_90/" style="text-decoration:none !important; border-bottom:0 !important;">
  <img alt="State of AI Report" src="https://img.shields.io/badge/State of AI Report--white?style=social&logo=googledocs&&logoColor=313554">
</a>
<a href="https://venturebeat.com/ai/ai2s-molmoact-model-thinks-in-3d-to-challenge-nvidia-and-google-in-robotics-ai" style="text-decoration:none !important; border-bottom:0 !important;">
  <img alt="VentureBeat" src="https://img.shields.io/badge/VentureBeat--white?style=social&logo=googledocs&&logoColor=313554">
</a>
<a href="https://www.geekwire.com/2025/ai2-unveils-molmoact-an-open-source-robotics-system-that-reasons-in-3d-and-adjusts-on-the-fly/" style="text-decoration:none !important; border-bottom:0 !important;">
  <img alt="GeekWire" src="https://img.shields.io/badge/GeekWire--white?style=social&logo=googledocs&&logoColor=313554">
</a>
<a href="https://www.businesswire.com/news/home/20250812422452/en/Ai2-Unveils-MolmoAct-a-New-Class-of-AI-Model-That-Reasons-in-3D-Space" style="text-decoration:none !important; border-bottom:0 !important;">
  <img alt="Business Wire" src="https://img.shields.io/badge/Business Wire--white?style=social&logo=googledocs&&logoColor=313554">
</a>
<a href="https://www.therobotreport.com/ai2-says-new-molmoact-7b-model-brings-ai-into-the-physical-world/" style="text-decoration:none !important; border-bottom:0 !important;">
  <img alt="The Robot Report" src="https://img.shields.io/badge/The Robot Report--white?style=social&logo=googledocs&&logoColor=313554">
</a>
<a href="https://siliconangle.com/2025/08/12/ai2-releases-open-ai-model-allows-robots-plan-movements-3d-space/" style="text-decoration:none !important; border-bottom:0 !important;">
  <img alt="SiliconANGLE" src="https://img.shields.io/badge/SiliconANGLE--white?style=social&logo=googledocs&&logoColor=313554">
</a>
</div>
</div>
<div class='paper-box'><div class='paper-box-image'><div><video id="2025sam2act" src='images/2025sam2act.mp4' alt="sym" width="100%" autoplay muted loop playsinline preload="auto" onloadedmetadata="this.defaultPlaybackRate=3;this.playbackRate=3" onplay="this.playbackRate=3" onratechange="if(this.playbackRate!==3)this.playbackRate=3"></video></div></div>
<div class='paper-box-text' markdown="1">
**SAM2Act: Integrating Visual Foundation Model with A Memory Architecture for Robotic Manipulation**
<br><ins>Haoquan Fang</ins>, Markus Grotz, Wilbert Pumacay, Yi Ru Wang, Dieter Fox<sup>‚Ä°</sup>, Ranjay Krishna<sup>‚Ä°</sup>, Jiafei Duan<sup>‚Ä°</sup>
<br>*International Conference on Machine Learning (ICML) 2025*
<br>*WRL @ ICLR 2025 <span style="color:red;">Oral Presentation</span>*
<br>*FMEA @ CVPR 2025 <span style="color:red;">Oral Presentation</span>*
<br>*RemembeRL @ CoRL 2025 <span style="color:red;">Best Paper Award</span>*
<br>
<a href="https://sam2act.github.io" style="text-decoration:none !important; border-bottom:0 !important;">
  <img alt="Website" src="https://img.shields.io/badge/Website--white?style=social&logo=googlechrome">
</a>
<a href="https://api.semanticscholar.org/arXiv:2501.18564" style="text-decoration:none !important; border-bottom:0 !important;">
  <img alt="Paper" src="https://img.shields.io/badge/dynamic/json?style=social&logo=semanticscholar&label=Paper&url=https%3A%2F%2Fhq-fang.github.io%2Fcitations%2Fcites_2501.18564.json&query=%24&suffix=%20%E2%80%9D&cacheSeconds=86400&v=2">
</a>
<a href="https://github.com/sam2act/sam2act" style="text-decoration:none !important; border-bottom:0 !important;">
  <img alt="Code"
       src="https://img.shields.io/badge/dynamic/json?style=social&label=Code&logo=github&query=%24.stargazers_count&url=https%3A%2F%2Fapi.github.com%2Frepos%2Fsam2act%2Fsam2act&suffix=%20‚≠ê&cacheSeconds=3600">
</a>
<a href="https://huggingface.co/datasets/hqfang/memorymench" style="text-decoration:none !important; border-bottom:0 !important;">
  <img alt="MemoryBench" src="https://img.shields.io/badge/MemoryBench--white?style=social&logo=huggingface&logoColor=yellow">
</a>
<a href="https://x.com/hq_fang/status/1885105252971798682" style="text-decoration:none !important; border-bottom:0 !important;">
  <img alt="Post" src="https://img.shields.io/badge/Post--white?style=social&logo=x">
</a>
<a href="https://hai-production.s3.amazonaws.com/files/hai_ai_index_report_2025.pdf#page=151" style="text-decoration:none !important; border-bottom:0 !important;">
  <img alt="AI Index Report" src="https://img.shields.io/badge/AI Index Report--white?style=social&logo=googledocs&&logoColor=313554">
</a>
</div>
</div>
<div class='paper-box'><div class='paper-box-image'><div><video id="2026rdvla" src='images/2026rdvla.mp4' alt="sym" width="100%" autoplay muted loop playsinline preload="auto" onloadedmetadata="this.defaultPlaybackRate=3;this.playbackRate=3" onplay="this.playbackRate=3" onratechange="if(this.playbackRate!==3)this.playbackRate=3"></video></div></div>
<div class='paper-box-text' markdown="1">
**Recurrent-Depth VLA: Implicit Test-Time Compute Scaling of Vision-Language-Action Models via Latent Iterative Reasoning**
<br>Yalcin Tur, Jalal Naghiyev<sup>‚Ä†</sup>, <ins>Haoquan Fang<sup>‚Ä†</sup></ins>, Wei-Chuan Tsai, Jiafei Duan<sup>‚Ä°</sup>, Dieter Fox<sup>‚Ä°</sup>, Ranjay Krishna<sup>‚Ä°</sup>
<br>*arXiv 2026*
<br>
<a href="https://rd-vla.github.io/" style="text-decoration:none !important; border-bottom:0 !important;">
  <img alt="Website" src="https://img.shields.io/badge/Website--white?style=social&logo=googlechrome">
</a>
<a href="https://api.semanticscholar.org/arXiv:2602.07845" style="text-decoration:none !important; border-bottom:0 !important;">
  <img alt="Paper" src="https://img.shields.io/badge/Paper--white?style=social&logo=semanticscholar">
</a>
<!-- <a href="https://api.semanticscholar.org/arXiv:2602.07845" style="text-decoration:none !important; border-bottom:0 !important;">
  <img alt="Paper" src="https://img.shields.io/badge/dynamic/json?style=social&logo=semanticscholar&label=Paper&url=https%3A%2F%2Fhq-fang.github.io%2Fcitations%2Fcites_2602.07845.json&query=%24&suffix=%20%E2%80%9D&cacheSeconds=86400&v=2">
</a> -->
<a href="https://github.com/rd-vla/rd-vla" style="text-decoration:none !important; border-bottom:0 !important;">
  <img alt="Code"
       src="https://img.shields.io/badge/dynamic/json?style=social&label=Code&logo=github&query=%24.stargazers_count&url=https%3A%2F%2Fapi.github.com%2Frepos%2Frd-vla%2Frd-vla&suffix=%20‚≠ê&cacheSeconds=3600">
</a>
<a href="https://x.com/DJiafei/status/2021250248082092094" style="text-decoration:none !important; border-bottom:0 !important;">
  <img alt="Post" src="https://img.shields.io/badge/Post--white?style=social&logo=x">
</a>
</div>
</div>
<div class='paper-box'><div class='paper-box-image'><div><video id="2025pointarena" src='images/2025pointarena.mp4' alt="sym" width="100%" autoplay muted loop playsinline preload="auto" onloadedmetadata="this.defaultPlaybackRate=3;this.playbackRate=3" onplay="this.playbackRate=3" onratechange="if(this.playbackRate!==3)this.playbackRate=3"></video></div></div>
<div class='paper-box-text' markdown="1">
**PointArena: Probing Multimodal Grounding Through Language-Guided Pointing**
<br>Long Cheng\*, Jiafei Duan\*, Yi Ru Wang<sup>‚Ä†</sup>, <ins>Haoquan Fang<sup>‚Ä†</sup></ins>, Boyang Li<sup>‚Ä†</sup>, Yushan Huang, Elvis Wang, Ainaz Eftekhar, Jason Lee, Wentao Yuan, Rose Hendrix, Noah A. Smith, Fei Xia, Dieter Fox, Ranjay Krishna
<br>*arXiv 2025*
<br>
<a href="https://pointarena.github.io" style="text-decoration:none !important; border-bottom:0 !important;">
  <img alt="Website" src="https://img.shields.io/badge/Website--white?style=social&logo=googlechrome">
</a>
<a href="https://api.semanticscholar.org/arXiv:2505.09990" style="text-decoration:none !important; border-bottom:0 !important;">
  <img alt="Paper" src="https://img.shields.io/badge/dynamic/json?style=social&logo=semanticscholar&label=Paper&url=https%3A%2F%2Fhq-fang.github.io%2Fcitations%2Fcites_2505.09990.json&query=%24&suffix=%20%E2%80%9D&cacheSeconds=86400&v=2">
</a>
<a href="https://github.com/pointarena/pointarena" style="text-decoration:none !important; border-bottom:0 !important;">
  <img alt="Code"
       src="https://img.shields.io/badge/dynamic/json?style=social&label=Code&logo=github&query=%24.stargazers_count&url=https%3A%2F%2Fapi.github.com%2Frepos%2Fpointarena%2Fpointarena&suffix=%20‚≠ê&cacheSeconds=3600">
</a>
<a href="https://huggingface.co/datasets/PointArena/pointarena-data" style="text-decoration:none !important; border-bottom:0 !important;">
  <img alt="Dataset" src="https://img.shields.io/badge/Dataset--white?style=social&logo=huggingface&logoColor=yellow">
</a>
<a href="https://x.com/hq_fang/status/1924516643369107958" style="text-decoration:none !important; border-bottom:0 !important;">
  <img alt="Post" src="https://img.shields.io/badge/Post--white?style=social&logo=x">
</a>
</div>
</div>
<div class='paper-box'><div class='paper-box-image'><div><video id="2025failsafe" src='images/2025failsafe.mp4' alt="sym" width="100%" autoplay muted loop playsinline preload="auto" onloadedmetadata="this.defaultPlaybackRate=3;this.playbackRate=3" onplay="this.playbackRate=3" onratechange="if(this.playbackRate!==3)this.playbackRate=3"></video></div></div>
<div class='paper-box-text' markdown="1">
**FailSafe: Reasoning and Recovery from Failures in Vision-Language-Action Models**
<br>Zijun Lin, Jiafei Duan, <ins>Haoquan Fang</ins>, Dieter Fox, Ranjay Krishna, Cheston Tan, Bihan Wen
<br>*arXiv 2025*
<br>
<a href="https://jimntu.github.io/FailSafe/" style="text-decoration:none !important; border-bottom:0 !important;">
  <img alt="Website" src="https://img.shields.io/badge/Website--white?style=social&logo=googlechrome">
</a>
<a href="https://api.semanticscholar.org/arXiv:2510.01642" style="text-decoration:none !important; border-bottom:0 !important;">
  <img alt="Paper" src="https://img.shields.io/badge/dynamic/json?style=social&logo=semanticscholar&label=Paper&url=https%3A%2F%2Fhq-fang.github.io%2Fcitations%2Fcites_2510.01642.json&query=%24&suffix=%20%E2%80%9D&cacheSeconds=86400&v=2">
</a>
<a href="https://github.com/Jimntu/FailSafe_code" style="text-decoration:none !important; border-bottom:0 !important;">
  <img alt="Code"
       src="https://img.shields.io/badge/dynamic/json?style=social&label=Code&logo=github&query=%24.stargazers_count&url=https%3A%2F%2Fapi.github.com%2Frepos%2FJimntu%2FFailSafe_code&suffix=%20‚≠ê&cacheSeconds=3600">
</a>
<a href="https://huggingface.co/datasets/onepiece1999/failsafe_data" style="text-decoration:none !important; border-bottom:0 !important;">
  <img alt="Dataset" src="https://img.shields.io/badge/Dataset--white?style=social&logo=huggingface&logoColor=yellow">
</a>
<a href="https://x.com/DJiafei/status/1986136467002040625" style="text-decoration:none !important; border-bottom:0 !important;">
  <img alt="Post" src="https://img.shields.io/badge/Post--white?style=social&logo=x">
</a>
</div>
</div>

## üìñ Academic Services {#services}
- **Conference Reviewer**
  - International Conference on Learning Representations (ICLR) 2025 Workshops
  - Conference on Computer Vision and Pattern Recognition (CVPR) 2025 Workshops
  - International Conference on Robotics and Automation (ICRA) 2026
- **Workshop Organizer**
  - Workshop on 3D Vision Language Models for Robotic Manipulation ([Robo-3DVLM](https://robo-3dvlms.github.io/)) at CVPR 2025
  - Workshop on Embodied Reasoning in Action ([ERA](https://embodied-reasoning.github.io/)) at CVPR 2026
- **Teaching Assistant & Grader**
  - UW CSE 493G1/599G1: Deep Learning (Undergrad/Grad, [Spring 2025](https://courses.cs.washington.edu/courses/cse493g1/25sp/), [Winter 2026](https://courses.cs.washington.edu/courses/cse493g1/26wi/))
  - UW STAT 311: Elements of Statistical Methods (Undergrad, Autumn 2025, Winter 2026)

## üí¨ Presentations {#talks}
All presentations below were delivered in person by me, either solo or with coauthors.
- **Genesis AI**, *Invited Talk on MolmoAct*
  - Hosted by [Johnson Wang](https://zswang666.github.io/), Aug 2025, San Carlos, CA
- **Google DeepMind**, *Invited Talk on MolmoAct*
  - Hosted by [Ted Xiao](https://tedxiao.me/), Aug 2025, Mountain View, CA
- **Allen Institute for AI**, *Company Demo on MolmoAct*
  - Hosted by [Ali Farhadi](https://www.cs.washington.edu/people/faculty/ali-farhadi/), Jun 2025, Seattle, WA
- **FMEA @ CVPR 2025**, *Oral Presentation on SAM2Act*
  - Hosted by [Manling Li](https://limanling.github.io/), Jun 2025, Nashville, TN
- **UW RAIVN Lab**, *Group Talk on SAM2Act*
  - Hosted by [Ranjay Krishna](https://ranjaykrishna.com/), Feb 2025, Seattle, WA
- **UW RSE Lab**, *Group Talk on SAM2Act*
  - Hosted by [Dieter Fox](https://homes.cs.washington.edu/~fox/), Feb 2025, Seattle, WA

## üéì Education {#education}
- **University of Washington**, *2022 - 2026 (Expected)*
<br>*Bachelor of Science in Computer Science with Honors*
<br>*Bachelor of Science in Statistics*
<br>*Minor in Mathematics*
  - GPA: 3.99/4.00, Dean‚Äôs Lists, Best Senior Thesis Award Honorable Mention, CSE Award for Excellence Scholarship, CRA Outstanding Undergraduate Researcher Award Awardee

## üíº Industrial Experiences {#work}
- **Allen Institute for AI**
<br>*Student Researcher, Nov 2025 - Present*
<br>*Student Collaborator, Apr 2025 - Oct 2025*
  - Worked with the [Perceptual Reasoning and Interaction Research (PRIOR)](https://prior.allenai.org/) Team
  - Mentored by Prof Ranjay Krishna, Prof Ali Farhadi, and Dr Jiafei Duan
- **Nokia Shanghai Bell**
<br>*Research Engineer Intern, Jul 2023 - Aug 2023*
  - Mentored by Dr Quan Wang, Explored on [COSMIC](https://cosmic-sizing.org/) Report Generation

## üèÖ Awards {#awards}
- [CRA Outstanding Undergraduate Researcher Award](https://cra.org/crn/2026/01/2025-26-cra-outstanding-undergraduate-researcher-award-recipients-announced/) Awardee 2026 (8 Awardees in North America)
- Best Paper Award at [RemembeRL @ CoRL 2025](https://rememberl-corl25.github.io/)
- Best Paper Award Runner-up at [Rational Robots @ CoRL 2025](https://rational-robots.github.io/)
- UW [CSE Award for Excellence Scholarship](https://www.cs.washington.edu/academics/undergraduate/student-resources/scholarships-awards/#allen) 2025
- UW CSE [Best Senior Thesis Award](https://www.cs.washington.edu/academics/undergraduate/student-resources/scholarships-awards/senior-thesis-award/) Honorable Mention 2025 (2nd Place)
- Kaggle Competitions Expert (Top 0.85% Highest)
- Kaggle - LLM Science Exam, Silver Medal (Top 4%) [[Website]](https://www.kaggle.com/competitions/kaggle-llm-science-exam/discussion/446303/)
- RSNA Screening Mammography Breast Cancer Detection, Solo Silver Medal (Top 5%)
